// Ejemplo donde ChatPromptTemplate es superior a PromptTemplate
// Caso de uso: Chatbot con memoria conversacional

import { StringOutputParser } from "@langchain/core/output_parsers";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { ChatDeepSeek } from "@langchain/deepseek";
import inquirer from "inquirer";

const model = new ChatDeepSeek({
  modelName: "deepseek-chat",
  temperature: 0.7,
  maxTokens: 200,
  apiKey: process.env.DEEPSEEK_API_KEY,
});

// ğŸŒŸ ChatPromptTemplate permite manejar mÃºltiples tipos de mensajes
// y mantener el contexto conversacional
const conversationalTemplate = ChatPromptTemplate.fromMessages([
  ["system", "Eres un asistente personal amigable llamado Alex. Tienes personalidad divertida y recuerdas todo lo que el usuario te dice durante la conversaciÃ³n. Siempre haces referencias a conversaciones anteriores cuando es relevante."],
  // â­ Esta es la clave: podemos agregar mensajes dinÃ¡micamente al historial
  ["placeholder", "{chat_history}"],
  ["human", "{input}"]
]);

// Crear la cadena de procesamiento
const conversationChain = conversationalTemplate.pipe(model).pipe(new StringOutputParser());

// Array para mantener el historial de la conversaciÃ³n
let chatHistory = [];

// FunciÃ³n para agregar mensajes al historial
function addToHistory(role, content) {
  chatHistory.push([role, content]);
}

// FunciÃ³n principal del chat conversacional
async function startConversationalChat() {
  console.log("ğŸ¤– Â¡Hola! Soy Alex, tu asistente personal con memoria.\n");
  console.log("ğŸ’­ Puedo recordar todo lo que hablemos en esta sesiÃ³n.\n");
  console.log("ğŸ’¬ Escribe 'salir' para terminar la conversaciÃ³n.\n");
  
  while (true) {
    const { input } = await inquirer.prompt([
      {
        type: 'input',
        name: 'input',
        message: 'TÃº:',
      }
    ]);
    
    if (input.toLowerCase() === 'salir') {
      console.log("ğŸ¤– Alex: Â¡Fue genial hablar contigo! Espero que nos veamos pronto. ğŸ‘‹");
      break;
    }
    
    if (input.trim() === '') {
      console.log("âš ï¸  Por favor, escribe algo para continuar la conversaciÃ³n.\n");
      continue;
    }
    
    try {
      console.log("ğŸ¤– Alex estÃ¡ pensando...");
      
      // â­ AquÃ­ es donde ChatPromptTemplate brilla:
      // Pasamos todo el historial de la conversaciÃ³n
      const response = await conversationChain.invoke({
        chat_history: chatHistory,
        input: input
      });
      
      // Agregar la pregunta del usuario y la respuesta del bot al historial
      addToHistory("human", input);
      addToHistory("assistant", response.trim());
      
      console.log(`ğŸ¤– Alex: ${response.trim()}\n`);
      
    } catch (error) {
      console.error("âŒ Error en la conversaciÃ³n:", error.message);
      console.log("ğŸ”„ Intenta de nuevo.\n");
    }
  }
}

// FunciÃ³n para demostrar por quÃ© PromptTemplate NO funcionarÃ­a aquÃ­
function explainWhyPromptTemplateWouldntWork() {
  console.log(`
ğŸ¯ Â¿Por quÃ© ChatPromptTemplate es superior aquÃ­?

âŒ Con PromptTemplate:
- Solo maneja un string simple
- No puede estructurar conversaciones con roles (system, human, assistant)
- No puede insertar historial dinÃ¡mico de mensajes
- PerderÃ­a todo el contexto conversacional

âœ… Con ChatPromptTemplate:
- Maneja mÃºltiples tipos de mensajes (system, human, assistant)
- El placeholder {chat_history} permite insertar mensajes dinÃ¡micamente
- Mantiene la estructura conversacional que los LLM necesitan
- El modelo entiende el contexto completo de la conversaciÃ³n

ğŸ§  Resultado: El bot recuerda todo y puede hacer referencias a conversaciones anteriores!
  `);
}

// Mostrar explicaciÃ³n y luego iniciar el chat
explainWhyPromptTemplateWouldntWork();
console.log("Presiona Enter para comenzar la demostraciÃ³n...");
await inquirer.prompt([{ type: 'input', name: 'continue', message: '' }]);

startConversationalChat().catch(console.error);
